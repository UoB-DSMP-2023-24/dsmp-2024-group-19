{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tapes and lob data\n",
    "\n",
    "do this for each day otherwise memory (RAM) exceeds most computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "from fast_tools import get_data, get_data_gen\n",
    "\n",
    "#data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "Remove outliers from lob and create an additional columns noting this\n",
    "\n",
    "FFill tapes data to get the most up to date tapes price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\r"
     ]
    }
   ],
   "source": [
    "# code\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "def get_tapes_window(tapes):\n",
    "    dt = 60*60 # in seconds\n",
    "    #stds = []\n",
    "    #means = []\n",
    "    w_bids = []\n",
    "    w_asks = []\n",
    "\n",
    "    t_start = 0\n",
    "    start_time = 0\n",
    "    z = 3.29 # 99.9%\n",
    "\n",
    "    outside = []\n",
    "    while True:\n",
    "        end_time = start_time + dt\n",
    "        t_end = t_start\n",
    "        rolling_tapes = []\n",
    "        while tapes[t_end, 0] < end_time:\n",
    "            rolling_tapes += [tapes[t_end, 1]] * int(tapes[t_end,2])\n",
    "            t_end += 1\n",
    "\n",
    "        mean = np.mean(rolling_tapes)\n",
    "        std = np.std(rolling_tapes)\n",
    "        #means.append(mean)\n",
    "\n",
    "        w_bid = mean - std * z\n",
    "        w_ask = mean + std * z\n",
    "        w_bids.append(w_bid)\n",
    "        w_asks.append(w_ask)\n",
    "\n",
    "        # look one minute a head\n",
    "        local_end = t_end\n",
    "        future_tapes = []\n",
    "        while tapes[local_end, 0] < end_time + 60:\n",
    "            future_tapes += [tapes[local_end, 1]] * int(tapes[local_end,2])\n",
    "            local_end += 1\n",
    "\n",
    "        future_tapes = np.array(future_tapes)\n",
    "\n",
    "        n_above = len(np.where(future_tapes > w_ask)[0])\n",
    "        n_below = len(np.where(future_tapes < w_bid)[0])\n",
    "        if end_time % 60 !=0:\n",
    "            raise ValueError\n",
    "        outside.append((end_time,n_above, n_below, len(future_tapes)))\n",
    "\n",
    "        start_time += 60\n",
    "        while tapes[t_start, 0] < start_time:\n",
    "            t_start += 1\n",
    "\n",
    "        end_time += dt\n",
    "        if end_time >= 8.5*60*60:\n",
    "            break\n",
    "\n",
    "    return outside, w_bids, w_asks\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_features(lob_data: np.array, \n",
    "                 lob_times: np.array, \n",
    "                 tapes: np.array, \n",
    "                 time_step_s: int, \n",
    "                 window_data: np.array,\n",
    "                 ab_weight = 1, \n",
    "                 median = False, \n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Calculate features from LOB and Tapes data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lob_data : np.array\n",
    "        Array containing the limit order book (LOB) data.\n",
    "    lob_times : np.array\n",
    "        Array containing timestamps for the LOB data.\n",
    "    tapes : np.array\n",
    "        Array containing Tapes data.\n",
    "    time_step_s : int\n",
    "        Time step in seconds for calculating features.\n",
    "    ab_weight : float, optional\n",
    "        Weight parameter for alpha and beta calculations, by default 1.\n",
    "    median : bool, optional\n",
    "        Whether to calculate features using median instead of mean, by default False.\n",
    "    cas_cbs_window : int, optional\n",
    "        Size of the window for calculating CAS and CBS, by default 800.\n",
    "    daily_data : bool, optional\n",
    "        Whether to calculate daily features, by default False.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - feat_arr: np.array\n",
    "            Array containing feature values.\n",
    "        - features: list\n",
    "            List of feature names.\n",
    "        - daily_arr: dict or None\n",
    "            Dictionary containing daily features or None if daily_data=False.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows = int((8.5 * 60 * 60) / time_step_s)                         # define number of rows of output array\n",
    "    features = [\"MP\",\"HIBID\",\"LOASK\",\"AP\",\"WBP\",\"WAP\",                  # define features\n",
    "                \"TCBS\",\"TCAS\",\"AWS\",\"VOL\",\"GAP\",\"SPREAD\",\n",
    "                \"ALPHA\", \"BETA\", \"ZETA\", \"ENDT\", \n",
    "                \"PSTD\", \"LOWIN\", \"HIWIN\", \"nUoD\"]\n",
    "    n_features = len(features)                                          # define number of features\n",
    "\n",
    "    feat_arr = np.zeros((n_rows, n_features), dtype=np.float64)         # array to hold feature values\n",
    "    \n",
    "    LA_HB_a_b = np.zeros((lob_data.shape[0]+1, 4), dtype = np.float64)  # array holding the LOASK, HIBID,\n",
    "                                                                        # alpha, beta, values \n",
    "\n",
    "    for i in prange(lob_data.shape[0]):                                 # iterates over the LOB to fill\n",
    "        row = lob_data[i]                                               # LA_HB_a_b values\n",
    "        \n",
    "        neg_ind = np.where(row < 0)[0]                                  # locate bid and ask prices (indicies)\n",
    "        pos_ind = np.where(row > 0)[0]\n",
    "        \n",
    "        if len(neg_ind) == 0:                                           # assign HIBID, np.nan if no values\n",
    "            LA_HB_a_b[i][1] = np.nan\n",
    "        else:\n",
    "            LA_HB_a_b[i][1] = max(neg_ind) + 1 \n",
    "\n",
    "        if len(pos_ind) == 0:                                           # assign HIBID, np.nan if no values\n",
    "            LA_HB_a_b[i][0] = np.nan\n",
    "        else:\n",
    "            LA_HB_a_b[i][0] = min(pos_ind) + 1\n",
    "\n",
    "        mid_price = (LA_HB_a_b[i][0] + LA_HB_a_b[i][1]) / 2             # calculate mid_price for alpha/beta calculations\n",
    "\n",
    "        if np.isnan(mid_price):\n",
    "            alpha = np.nan\n",
    "            beta = np.nan\n",
    "        else:                                                           # calculate alpha/beta using ab_weight var\n",
    "            beta = 0\n",
    "            for ind in neg_ind:\n",
    "                beta += (-1 * row[ind]) / ((mid_price - (ind + 1)) + ab_weight)\n",
    "    \n",
    "            alpha = 0\n",
    "            for ind in pos_ind:\n",
    "                alpha += row[ind] / (((ind + 1) - mid_price) + ab_weight)\n",
    "                \n",
    "\n",
    "        LA_HB_a_b[i][2] = alpha\n",
    "        LA_HB_a_b[i][3] = beta\n",
    "        \n",
    "    max_lob = lob_data.shape[0] - 1                                      # define max indicies for lob\n",
    "    max_tapes = tapes.shape[0] - 1                                       # define max indicies for tapes\n",
    "    \n",
    "    start_time = 0                                                       # define start time\n",
    "    lob_start = 0                                                        # define start index for lob\n",
    "    tapes_start = 0                                                      # define start index for tapes\n",
    "    \n",
    "    cas = np.zeros(800, dtype = np.int16)                                # define an array to hold CAS values\n",
    "    cbs = np.zeros(800, dtype = np.int16)                                # define an array to hold CBS values\n",
    "    for row_i in range(n_rows):\n",
    "        end_time = start_time + time_step_s                              # move to next time step\n",
    "        lob_end = lob_start\n",
    "        tapes_end = tapes_start\n",
    "\n",
    "        # get lob end index\n",
    "        while lob_times[lob_end] < end_time and lob_end < max_lob:       # move lob indicies to end time\n",
    "            lob_end += 1\n",
    "        \n",
    "        # get tapes end index\n",
    "        while tapes[tapes_end][0] < end_time and tapes_end < max_tapes:  # move tapes indicies to end time\n",
    "            tapes_end += 1\n",
    "\n",
    "        # feature calculations\n",
    "        if tapes_start == tapes_end:                                     # if there is no tapes data\n",
    "            AP = np.nan                                                  # set tapes features to np.nan\n",
    "            VOL = np.nan\n",
    "            PSTD = np.nan\n",
    "            nUoD = np.nan\n",
    "        else:\n",
    "            tapes_slice = tapes[tapes_start:tapes_end]                   # extract tapes slice, calculate AP, VOL\n",
    "            tapes_list = []\n",
    "            \n",
    "            for row in tapes_slice:\n",
    "                for _ in range(int(row[2])):\n",
    "                    tapes_list.append(row[1])\n",
    "\n",
    "            tapes_list = np.array(tapes_list, dtype=np.int32)\n",
    "            AP = np.mean(tapes_list)\n",
    "            VOL = np.sum(tapes_slice[:,2])\n",
    "            PSTD = np.std(tapes_list)\n",
    "\n",
    "            tapes_price_diff = tapes_slice[:,1][1:] - tapes_slice[:,1][:-1]\n",
    "            n_ups = np.sum(tapes_price_diff > 0)\n",
    "            n_downs = np.sum(tapes_price_diff < 0)\n",
    "            nUoD = (n_ups + 1) / (n_downs + 1) - 1\n",
    "\n",
    "        if lob_start == lob_end:                                         # if there is no LOB data\n",
    "            MP = np.nan                                                  # set lob features to np.nan\n",
    "            HIBID = np.nan\n",
    "            LOASK = np.nan\n",
    "            SPREAD = np.nan\n",
    "            TCBS = np.nan\n",
    "            TCAS = np.nan\n",
    "            WBP = np.nan\n",
    "            WAP = np.nan\n",
    "            AWS = np.nan\n",
    "            ALPHA = np.nan\n",
    "            BETA = np.nan\n",
    "            ZETA = np.nan  \n",
    "        else:\n",
    "            lob_slice = lob_data[lob_start:lob_end]                       # extract slices of data \n",
    "            LA_HB_a_b_slice = LA_HB_a_b[lob_start:lob_end]                \n",
    "\n",
    "            # midprice_calcs, alpha, beta\n",
    "            if median:                                                    # calculate price features\n",
    "                HIBID = np.median(LA_HB_a_b_slice[:,1])                   # using median if set to true\n",
    "                LOASK = np.median(LA_HB_a_b_slice[:,0])\n",
    "                ALPHA = np.median(LA_HB_a_b_slice[:,2])\n",
    "                BETA = np.median(LA_HB_a_b_slice[:,3])\n",
    "            else:\n",
    "                HIBID = np.nanmean(LA_HB_a_b_slice[:,1])\n",
    "                LOASK = np.nanmean(LA_HB_a_b_slice[:,0])\n",
    "                ALPHA = np.nanmean(LA_HB_a_b_slice[:,2])\n",
    "                BETA = np.nanmean(LA_HB_a_b_slice[:,3])\n",
    "\n",
    "            MP = (HIBID + LOASK) / 2\n",
    "            SPREAD = LOASK - HIBID\n",
    "            ZETA = BETA - ALPHA\n",
    "\n",
    "            if HIBID >= LOASK:\n",
    "                print(\"WARNING: HIBID >= LOASK\")\n",
    "\n",
    "            # consolidated calcs\n",
    "            cas[:] = 0                                                      # reset cas, cbs arrays for new data\n",
    "            cbs[:] = 0 \n",
    "\n",
    "            window_index = np.where(window_data[:,0] == end_time)[0]\n",
    "            if len(window_index) == 1:\n",
    "                w_bid = window_data[window_index[0], 4]\n",
    "                w_ask = window_data[window_index[0], 5]\n",
    "                LOWIN = window_data[window_index[0], 1]\n",
    "                HIWIN = window_data[window_index[0], 2]\n",
    "            else:\n",
    "                w_bid = MP - 100\n",
    "                w_ask = MP + 100\n",
    "                LOWIN = 0\n",
    "                HIWIN = 0\n",
    "\n",
    "            for ci in prange(int(np.floor(w_bid) - 1), int(np.ceil(w_ask) + 2)):\n",
    "                # can optimise with LOASK AND HIBID here\n",
    "                                                                        # only calculate cbs between window left of MP\n",
    "                cbs_vec = lob_slice[:,ci].copy() * -1                   # and less than LOASK + 100 for efficiency\n",
    "                cbs_vec[cbs_vec <= 0] = 0                               # idk if this breaks things for efficiency ?:\n",
    "                cbs[ci] = np.sum(np.abs(np.diff(cbs_vec))) + cbs_vec[0]\n",
    "\n",
    "                                                                        # only calculate cas between window right of MP\n",
    "                cas_vec = lob_slice[:,ci].copy()                        # and greater than HIBID - 100 for efficiency\n",
    "                cas_vec[cas_vec <= 0] = 0                               # idk if this breaks things for efficiency ?:\n",
    "                cas[ci] = np.sum(np.abs(np.diff(cas_vec))) + cas_vec[0]\n",
    "\n",
    "            TCBS = np.sum(cbs)                                              # Total CBS\n",
    "            TCAS = np.sum(cas)                                              # Total CAS\n",
    "\n",
    "            if TCBS == 0:                                                   # Calculate WBP, np.nan if no activity\n",
    "                WBP = np.nan\n",
    "            else:\n",
    "                WBP = 0\n",
    "                for ci in prange(800):\n",
    "                    WBP += (ci + 1) * (cbs[ci] / TCBS)\n",
    "\n",
    "            if TCAS == 0:                                                   # Calculate WAP, np.nan if no activity\n",
    "                WAP = np.nan\n",
    "            else:\n",
    "                WAP = 0\n",
    "                for ci in prange(800):\n",
    "                    WAP += (ci + 1) * (cas[ci] / TCAS)\n",
    "\n",
    "            AWS = WAP - WBP                                                 # Activity weighted spread calc\n",
    "\n",
    "        # feature setting\n",
    "        feat_arr[row_i][features.index(\"AP\")] = AP                          # set the values to the feat_arr\n",
    "        feat_arr[row_i][features.index(\"VOL\")] = VOL\n",
    "        feat_arr[row_i][features.index(\"MP\")] = MP\n",
    "        feat_arr[row_i][features.index(\"HIBID\")] = HIBID\n",
    "        feat_arr[row_i][features.index(\"LOASK\")] = LOASK\n",
    "        feat_arr[row_i][features.index(\"SPREAD\")] = SPREAD\n",
    "        feat_arr[row_i][features.index(\"TCAS\")] = TCAS\n",
    "        feat_arr[row_i][features.index(\"TCBS\")] = TCBS\n",
    "        feat_arr[row_i][features.index(\"WAP\")] = WAP\n",
    "        feat_arr[row_i][features.index(\"WBP\")] = WBP\n",
    "        feat_arr[row_i][features.index(\"AWS\")] = AWS\n",
    "        feat_arr[row_i][features.index(\"ALPHA\")] = ALPHA\n",
    "        feat_arr[row_i][features.index(\"BETA\")] = BETA\n",
    "        feat_arr[row_i][features.index(\"ZETA\")] = ZETA\n",
    "        feat_arr[row_i][features.index(\"GAP\")] = MP - AP\n",
    "        feat_arr[row_i][features.index(\"ENDT\")] = end_time\n",
    "        feat_arr[row_i][features.index(\"PSTD\")] = PSTD\n",
    "        feat_arr[row_i][features.index(\"LOWIN\")] = LOWIN\n",
    "        feat_arr[row_i][features.index(\"HIWIN\")] = HIWIN\n",
    "        feat_arr[row_i][features.index(\"nUoD\")] = nUoD\n",
    "\n",
    "\n",
    "        # adjust start times\n",
    "        start_time = end_time                                                # Set the next start times and \n",
    "        lob_start = lob_end                                                  # indicies to the last end times / indicies\n",
    "        tapes_start = tapes_end\n",
    "\n",
    "    return feat_arr, features\n",
    "\n",
    "c = 0\n",
    "all_features = []\n",
    "for lob, lob_times, tapes in get_data_gen():\n",
    "    print(c, end = \"\\r\")\n",
    "    outside, w_bids, w_asks = get_tapes_window(tapes)\n",
    "    window_data = np.zeros((len(outside), 6), dtype = float)\n",
    "    for i in range(len(outside)):\n",
    "        window_data[i][:4] = outside[i]\n",
    "        window_data[i][4] = w_bids[i]\n",
    "        window_data[i][5] = w_asks[i]\n",
    "\n",
    "    features = get_features(lob, lob_times, tapes, 10, window_data)\n",
    "    all_features.append(features)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = [pd.DataFrame(fa, columns=f) for fa, f in all_features]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"WMP\"] = (df[\"WBP\"] + df[\"WAP\"]) / 2\n",
    "    df[\"MP_diff(y)\"] = df[\"MP\"].diff(1).shift(-1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from LOB and Tapes\n",
    "\n",
    "get mean, std, trend, delta from 60 min segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "features = [\"TCAS\", \"TCBS\", \"ALPHA\", \"BETA\", \"ZETA\",\n",
    "            \"WMP\", \"AWS\", \"VOL\", \"GAP\", \"nUoD\", \"PSTD\"]\n",
    "\n",
    "sum_features = [\"LOWIN\", \"HIWIN\"]\n",
    "\n",
    "y_feat = \"MP_diff(y)\"\n",
    "\n",
    "X_dfs = []\n",
    "\n",
    "\n",
    "\n",
    "for c, df in enumerate(dfs):\n",
    "    print(c, end = \"\\r\")\n",
    "\n",
    "    X_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(df) - 60):\n",
    "        train_segment = df[i:i+60]\n",
    "        test_segment = df[i+60:i+61]\n",
    "\n",
    "        row = {}\n",
    "        for f in features:\n",
    "            # std\n",
    "            row[f+\"_std\"] = np.std(train_segment[f])\n",
    "            # mean\n",
    "            if f != \"WMP\": # exclude mean midprice\n",
    "                row[f+\"_mean\"] = np.mean(train_segment[f])\n",
    "                row[f+\"_close\"] = train_segment[f].iloc[-1]\n",
    "            # delta\n",
    "            row[f+\"_delta\"] = train_segment[f].iloc[-1] - train_segment[f].iloc[0]\n",
    "            # trend\n",
    "            row[f+\"_corr\"] = np.corrcoef(np.arange(len(train_segment[f])), train_segment[f].to_numpy())[0, 1]\n",
    "\n",
    "        for f in sum_features:\n",
    "            row[f+\"_sum\"] = np.sum(train_segment[f])\n",
    "\n",
    "        row[y_feat] = test_segment[y_feat].iloc[0]\n",
    "        row[\"ENDT\"] = test_segment[\"ENDT\"]\n",
    "\n",
    "        X_df = pd.concat([X_df, pd.DataFrame(row)])\n",
    "\n",
    "    X_dfs.append(X_df)\n",
    "\n",
    "X_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = pd.concat(X_dfs[:], axis = 0)\n",
    "merged_dfs.to_csv(\"final_merged_10s.csv\")\n",
    "\n",
    "clean_merge_dfs = merged_dfs.dropna(how=\"any\")\n",
    "\n",
    "x_features = list(X_df)\n",
    "x_features.remove(y_feat)\n",
    "\n",
    "X = clean_merge_dfs[x_features]\n",
    "y = 1 / (1 + np.exp(-clean_merge_dfs[y_feat]/4))\n",
    "\n",
    "baseline_mae = np.mean(np.abs(y - 0.5))\n",
    "print(\"Baseline MAE: \", baseline_mae)\n",
    "print(\"Samples: \", len(merged_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X contains your input features and y contains your target values\n",
    "# X shape: (number of samples, 35)\n",
    "\n",
    "# Step 1: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# Step 2: Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DTR = DecisionTreeRegressor(max_leaf_nodes=10)\n",
    "DTR.fit(X_train, y_train)\n",
    "\n",
    "ypred = DTR.predict(X_test)\n",
    "print(mean_absolute_error(ypred, y_test))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(DTR, feature_names=x_features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(ypred)\n",
    "plt.plot(y_test.to_numpy())\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(x_features, DTR.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svc = SVR()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "ypred = SVR.predict(X_test)\n",
    "print(mean_absolute_error(ypred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "def create_regression_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "        layers.Dropout(0.2),  # Add Dropout layer\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid')  # Output layer with single neuron for regression\n",
    "    ])\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=0.05)\n",
    "    model.compile(optimizer=opt, loss='mean_absolute_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "input_shape = (len(x_features),)\n",
    "regression_model = create_regression_model(input_shape)\n",
    "\n",
    "# Display model architecture\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "history = regression_model.fit(X_train_normalized, y_train, epochs=1000, batch_size=512, validation_split=0.5, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.axhline(baseline_mae, c = \"black\", linestyle = \"--\")\n",
    "plt.show()\n",
    "\n",
    "test_loss = regression_model.evaluate(X_test_normalized, y_test)\n",
    "model_filename = f\"regression_model_test_score_{test_loss[-1]:.4f}.keras\"\n",
    "regression_model.save(model_filename)\n",
    "# Print the test loss\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save features\n",
    "\n",
    "for use in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs.to_csv(\"final_merged_5s.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_dfs = pd.read_csv(\"final_merged_15s.csv\", index_col=0)\n",
    "plt.figure(figsize=(50,40))\n",
    "sns.heatmap(merged_dfs.corr(), vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "merged_dfs = pd.read_csv(\"final_merged_15s.csv\", index_col=0)\n",
    "x_features = list(merged_dfs)\n",
    "x_features.remove(\"MP_diff(y)\")\n",
    "x_features.remove(\"LOWIN_sum\")\n",
    "\n",
    "clean_dfs = merged_dfs.dropna(how=\"any\")\n",
    "\n",
    "for col in x_features:\n",
    "    std = clean_dfs[col].std()\n",
    "    mean = clean_dfs[col].mean()\n",
    "\n",
    "    clean_dfs[col] = (clean_dfs[col] - mean) / std\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca_results = pca.fit_transform(clean_dfs[x_features])\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c = np.log(clean_dfs[\"MP_diff(y)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
